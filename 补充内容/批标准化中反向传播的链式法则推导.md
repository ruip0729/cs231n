![img](https://raw.githubusercontent.com/cs231n/cs231n.github.io/master/assets/a2/batchnorm_graph.png)

根据上图有如下公式
$$
\begin{align}

& \mu=\frac{1}{N}\sum_{k=1}^N x_k  &  v=\frac{1}{N}\sum_{k=1}^N (x_k-\mu)^2 \\

& \sigma=\sqrt{v+\epsilon}     &  y_i=\frac{x_i-\mu}{\sigma}

\end{align}
$$
注意gamma和beta的存在，out = gamma*x_hat + beta



# 以下是反向传播中个人认为需要注意的点：

## 1. beta的梯度

在Batch Normalization中，平移参数beta的梯度之所以是 `np.sum(dout, axis=0)`，是因为：

损失函数 L 对 beta 的梯度公式为：
$$
\frac{\partial L}{\partial \beta} = \sum_{i=1}^N \frac{\partial L}{\partial \text{out}_i}
$$
因为 beta 对每一行样本的输出都有直接贡献，因此梯度需要对批量中所有样本的导数累加。



## 2. mean的梯度

#### （1）公式来源

前向传播中，输入 x 的规范化公式是：
$$
\hat{x} = \frac{x - \mu}{\sqrt{\text{var} + \epsilon}}
$$
这里，均值和方差分别为：
$$
\mu = \frac{1}{N} \sum_{i=1}^N x_i, \quad \text{var} = \frac{1}{N} \sum_{i=1}^N (x_i - \mu)^2
$$
损失函数 L 通过链式法则反向传播时，需要计算损失对均值 μ 的梯度 
$$
\frac{\partial L}{\partial \mu}
$$

#### （2）梯度计算分解

![img](https://raw.githubusercontent.com/cs231n/cs231n.github.io/master/assets/a2/batchnorm_graph.png)

根据该图像可知均值 μ 的梯度来源于两部分，及Y和v

1. **从规范化公式出发**（即Y）

   规范化数据 hat{x} 与均值 μ 的关系：
   $$
   \frac{\partial \hat{x}}{\partial \mu} = -\frac{1}{\sqrt{\text{var} + \epsilon}}
   $$
   损失对均值的梯度：
   $$
   \frac{\partial L}{\partial \mu} = \sum_{i=1}^N \frac{\partial L}{\partial \hat{x}_i} \cdot \frac{\partial \hat{x}_i}{\partial \mu} = \sum_{i=1}^N dx_{\hat{x}_i} \cdot \left(-\frac{1}{\sqrt{\text{var} + \epsilon}}\right)
   $$
   这对应代码中的第一部分：

   ```python
   np.sum(dx_hat * -1.0 / np.sqrt(var + eps), axis=0)
   ```

2. **通过方差传播的梯度**（即v）

   方差 var 的计算公式是：
   $$
   \text{var} = \frac{1}{N} \sum_{i=1}^N (x_i - \mu)
   $$
   方差的梯度传播到均值：
   $$
   \frac{\partial \text{var}}{\partial \mu} = \frac{1}{N} \sum_{i=1}^N \frac{\partial}{\partial \mu} \left((x_i - \mu)^2\right) = \frac{1}{N} \sum_{i=1}^N -2 (x_i - \mu)
   $$
   损失对均值的贡献：
   $$
   \frac{\partial L}{\partial \mu} = \frac{\partial L}{\partial \text{var}} \cdot \frac{\partial \text{var}}{\partial \mu}
   $$
   在代码中表示为：

   ```python
   dvar * np.sum(-2.0 * (x - mean), axis=0) / N
   ```

3. **合并梯度** 

   ```python
   dmean = np.sum(dx_hat * -1.0 / np.sqrt(var + eps), axis=0) + \
           dvar * np.sum(-2.0 * (x - mean), axis=0) / N
   ```



## 3. X的梯度

在 Batch Normalization中，输入 x 通过以下步骤进行处理，这里不用考虑gamma和beta：

1. **计算均值和方差**：
   $$
   \mu = \frac{1}{N} \sum_{i=1}^N x_i, \quad \sigma^2 = \frac{1}{N} \sum_{i=1}^N (x_i - \mu)^2
   $$
   

2. **规范化**：
   $$
   \hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}
   $$
   

在反向传播时，目标是计算损失 L 对 x 的梯度 
$$
\frac{\partial L}{\partial x}
$$
通过链式法则逐步分解计算。

#### （1）公式拆解

#### 1. 梯度传播链

通过链式法则，梯度可以分解为三部分：
$$
\frac{\partial L}{\partial x_i} = \frac{\partial L}{\partial \hat{x}_i} \cdot \frac{\partial \hat{x}_i}{\partial x_i} + \frac{\partial L}{\partial \sigma^2} \cdot \frac{\partial \sigma^2}{\partial x_i} + \frac{\partial L}{\partial \mu} \cdot \frac{\partial \mu}{\partial x_i}
$$

#### 2. 每部分梯度计算

1. **第一部分**，对于梯度
   $$
   \frac{\partial \hat{x}_i}{\partial x_i}
   $$
   有以下推导
   $$
   \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}} \quad \Rightarrow \quad \frac{\partial \hat{x}_i}{\partial x_i} = \frac{1}{\sqrt{\sigma^2 + \epsilon}}
   $$
   所以：
   $$
   \frac{\partial L}{\partial x_i} \bigg|_{\hat{x}_i} = \frac{\partial L}{\partial \hat{x}_i} \cdot \frac{1}{\sqrt{\sigma^2 + \epsilon}}
   $$

2. **第二部分**，对于梯度
   $$
   \frac{\partial \sigma^2}{\partial x_i}
   $$
   有以下推导
   $$
   \sigma^2 = \frac{1}{N} \sum_{j=1}^N (x_j - \mu)^2 \quad \Rightarrow \quad \frac{\partial \sigma^2}{\partial x_i} = \frac{2}{N} (x_i - \mu)
   $$
   损失梯度传播到方差的部分为：
   $$
   \frac{\partial L}{\partial x_i} \bigg|_{\sigma^2} = \frac{\partial L}{\partial \sigma^2} \cdot \frac{2}{N} (x_i - \mu)
   $$

3. **第三部分**，对于梯度
   $$
   \frac{\partial \mu}{\partial x_i}
   $$
   有以下推导
   $$
   \mu = \frac{1}{N} \sum_{j=1}^N x_j \quad \Rightarrow \quad \frac{\partial \mu}{\partial x_i} = \frac{1}{N}
   $$
   损失梯度传播到均值的部分为：
   $$
   \frac{\partial L}{\partial x_i} \bigg|_{\mu} = \frac{\partial L}{\partial \mu} \cdot \frac{1}{N}
   $$

#### （2）合并三部分

将所有梯度加总后，得到：
$$
\frac{\partial L}{\partial x_i} = \frac{\partial L}{\partial \hat{x}_i} \cdot \frac{1}{\sqrt{\sigma^2 + \epsilon}} + \frac{\partial L}{\partial \sigma^2} \cdot \frac{2}{N} (x_i - \mu) + \frac{\partial L}{\partial \mu} \cdot \frac{1}{N}
$$
对应代码中的实现：

```python
dx = dx_hat / np.sqrt(var + eps) + dvar * 2.0 * (x - mean) / N + dmean / N
```